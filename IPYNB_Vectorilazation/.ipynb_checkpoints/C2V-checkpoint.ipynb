{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1363cc0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vocabularies import VocabType\n",
    "from config import Config\n",
    "from interactive_predict import InteractivePredictor\n",
    "from model_base import Code2VecModelBase\n",
    "from tensorflow_model import Code2VecModel as model\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import time\n",
    "from typing import Dict, Optional, List, Iterable\n",
    "from collections import Counter\n",
    "from functools import partial\n",
    "\n",
    "from path_context_reader import PathContextReader, ModelInputTensorsFormer, ReaderInputTensors, EstimatorAction\n",
    "from common import common\n",
    "from vocabularies import VocabType\n",
    "from config import Config\n",
    "from model_base import Code2VecModelBase, ModelEvaluationResults, ModelPredictionResults\n",
    "\n",
    "from math import ceil\n",
    "from typing import Optional\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c608953e",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_TRAIN_EPOCHS = 10 #20\n",
    "SAVE_EVERY_EPOCHS = 1\n",
    "TRAIN_BATCH_SIZE = 512 #1024\n",
    "TEST_BATCH_SIZE = TRAIN_BATCH_SIZE\n",
    "TOP_K_WORDS_CONSIDERED_DURING_PREDICTION = 10\n",
    "NUM_BATCHES_TO_LOG_PROGRESS = 10\n",
    "NUM_TRAIN_BATCHES_TO_EVALUATE = 1800\n",
    "READER_NUM_PARALLEL_BATCHES = 6  # cpu cores [for tf.contrib.data.map_and_batch() in the reader]\n",
    "SHUFFLE_BUFFER_SIZE = 10000\n",
    "CSV_BUFFER_SIZE = 100 * 1024 * 1024  # 100 MB\n",
    "MAX_TO_KEEP = 10\n",
    "\n",
    "# model hyper-params\n",
    "MAX_CONTEXTS = 200\n",
    "MAX_TOKEN_VOCAB_SIZE = 13011\n",
    "MAX_TARGET_VOCAB_SIZE = 2612\n",
    "MAX_PATH_VOCAB_SIZE = 9114\n",
    "DEFAULT_EMBEDDINGS_SIZE = 128\n",
    "TOKEN_EMBEDDINGS_SIZE = DEFAULT_EMBEDDINGS_SIZE\n",
    "PATH_EMBEDDINGS_SIZE = DEFAULT_EMBEDDINGS_SIZE\n",
    "CODE_VECTOR_SIZE = PATH_EMBEDDINGS_SIZE + 2 *TOKEN_EMBEDDINGS_SIZE\n",
    "TARGET_EMBEDDINGS_SIZE = CODE_VECTOR_SIZE\n",
    "DROPOUT_KEEP_RATE = 0.75\n",
    "SEPARATE_OOV_AND_PAD = False\n",
    "\n",
    "train_steps_per_epoch=NUM_TRAIN_EXAMPLES / TRAIN_BATCH_SIZE\n",
    "\n",
    "# Automatically filled by `args`.\n",
    "PREDICT: bool = False   # TODO: update README;\n",
    "MODEL_SAVE_PATH: Optional[str] = None\n",
    "MODEL_LOAD_PATH: Optional[str] = None\n",
    "TRAIN_DATA_PATH_PREFIX: Optional[str] = None\n",
    "TEST_DATA_PATH: Optional[str] = ''\n",
    "RELEASE: bool = False\n",
    "EXPORT_CODE_VECTORS: bool = False\n",
    "SAVE_W2V: Optional[str] = None   # TODO: update README;\n",
    "SAVE_T2V: Optional[str] = None   # TODO: update README;\n",
    "VERBOSE_MODE: int = 0\n",
    "LOGS_PATH: Optional[str] = None\n",
    "DL_FRAMEWORK: str = 'tensorflow'  # in {'keras', 'tensorflow'}\n",
    "USE_TENSORBOARD: bool = False\n",
    "\n",
    "# Automatically filled by `Code2VecModelBase._init_num_of_examples()`.\n",
    "NUM_TRAIN_EXAMPLES: int = 0\n",
    "NUM_TEST_EXAMPLES: int = 0\n",
    "\n",
    "__logger: Optional[logging.Logger] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "441c31ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'vocassbs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_8160/1983748669.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mnum_batches_to_save_and_eval\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_steps_per_epoch\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mSAVE_EVERY_EPOCHS\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m train_reader = PathContextReader(vocabs=vocassbs,\n\u001b[0m\u001b[0;32m     10\u001b[0m                                     \u001b[0mmodel_input_tensors_former\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0m_TFTrainModelInputTensorsFormer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m                                     config=config, estimator_action=EstimatorAction.Train)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'vocassbs' is not defined"
     ]
    }
   ],
   "source": [
    "print('Starting training')\n",
    "start_time = time.time()\n",
    "\n",
    "batch_num = 0\n",
    "sum_loss = 0\n",
    "multi_batch_start_time = time.time()\n",
    "num_batches_to_save_and_eval = max(int(train_steps_per_epoch * SAVE_EVERY_EPOCHS), 1)\n",
    "\n",
    "train_reader = PathContextReader(vocabs=vocabs,\n",
    "                                    model_input_tensors_former=_TFTrainModelInputTensorsFormer(),\n",
    "                                    config=config, estimator_action=EstimatorAction.Train)\n",
    "input_iterator = tf.compat.v1.data.make_initializable_iterator(train_reader.get_dataset())\n",
    "input_iterator_reset_op = input_iterator.initializer\n",
    "input_tensors = input_iterator.get_next()\n",
    "\n",
    "optimizer, train_loss = _build_tf_training_graph(input_tensors)\n",
    "saver = tf.compat.v1.train.Saver(max_to_keep=MAX_TO_KEEP)\n",
    "\n",
    "print('Number of trainable params: {}'.format(\n",
    "    np.sum([np.prod(v.get_shape().as_list()) for v in tf.compat.v1.trainable_variables()])))\n",
    "for variable in tf.compat.v1.trainable_variables():\n",
    "    print(\"variable name: {} -- shape: {} -- #params: {}\".format(\n",
    "        variable.name, variable.get_shape(), np.prod(variable.get_shape().as_list())))\n",
    "\n",
    "_initialize_session_variables()\n",
    "\n",
    "if MODEL_LOAD_PATH:\n",
    "    _load_inner_model(self.sess)\n",
    "\n",
    "sess.run(input_iterator_reset_op)\n",
    "time.sleep(1)\n",
    "print('Started reader...')\n",
    "# run evaluation in a loop until iterator is exhausted.\n",
    "try:\n",
    "    while True:\n",
    "        # Each iteration = batch. We iterate as long as the tf iterator (reader) yields batches.\n",
    "        batch_num += 1\n",
    "\n",
    "        # Actual training for the current batch.\n",
    "        _, batch_loss = self.sess.run([optimizer, train_loss])\n",
    "\n",
    "        sum_loss += batch_loss\n",
    "        if batch_num % NUM_BATCHES_TO_LOG_PROGRESS == 0:\n",
    "            _trace_training(sum_loss, batch_num, multi_batch_start_time)\n",
    "            # Uri: the \"shuffle_batch/random_shuffle_queue_Size:0\" op does not exist since the migration to the new reader.\n",
    "            # self.print('Number of waiting examples in queue: %d' % self.sess.run(\n",
    "            #    \"shuffle_batch/random_shuffle_queue_Size:0\"))\n",
    "            sum_loss = 0\n",
    "            multi_batch_start_time = time.time()\n",
    "        if batch_num % num_batches_to_save_and_eval == 0:\n",
    "            epoch_num = int((batch_num / num_batches_to_save_and_eval) * SAVE_EVERY_EPOCHS)\n",
    "            model_save_path = MODEL_SAVE_PATH + '_iter' + str(epoch_num)\n",
    "            save(model_save_path)\n",
    "            print('Saved after %d epochs in: %s' % (epoch_num, model_save_path))\n",
    "            evaluation_results = self.evaluate()\n",
    "            evaluation_results_str = (str(evaluation_results).replace('topk', 'top{}'.format(\n",
    "                self.TOP_K_WORDS_CONSIDERED_DURING_PREDICTION)))\n",
    "            self.print('After {nr_epochs} epochs -- {evaluation_results}'.format(\n",
    "                nr_epochs=epoch_num,\n",
    "                evaluation_results=evaluation_results_str\n",
    "            ))\n",
    "except tf.errors.OutOfRangeError:\n",
    "    pass  # The reader iterator is exhausted and have no more batches to produce.\n",
    "\n",
    "print('Done training')\n",
    "\n",
    "if self.MODEL_SAVE_PATH:\n",
    "    _save_inner_model(self.MODEL_SAVE_PATH)\n",
    "    print('Model saved in file: %s' % self.MODEL_SAVE_PATH)\n",
    "\n",
    "elapsed = int(time.time() - start_time)\n",
    "print(\"Training time: %sH:%sM:%sS\\n\" % ((elapsed // 60 // 60), (elapsed // 60) % 60, elapsed % 60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81d30c19",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
