{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1363cc0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vocabularies import VocabType\n",
    "from config import Config\n",
    "from interactive_predict import InteractivePredictor\n",
    "from model_base import Code2VecModelBase\n",
    "from tensorflow_model import Code2VecModel as model\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import time\n",
    "from typing import Dict, Optional, List, Iterable\n",
    "from collections import Counter\n",
    "from functools import partial\n",
    "\n",
    "from path_context_reader import PathContextReader, ModelInputTensorsFormer, ReaderInputTensors, EstimatorAction\n",
    "from common import common\n",
    "from vocabularies import VocabType\n",
    "from config import Config\n",
    "from model_base import Code2VecModelBase, ModelEvaluationResults, ModelPredictionResults\n",
    "\n",
    "from math import ceil\n",
    "from typing import Optional\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c608953e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'vocabs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_13372/1749221202.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[0m__logger\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlogging\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLogger\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 50\u001b[1;33m \u001b[0mvocabs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvocabs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     51\u001b[0m \u001b[0mconfig\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[0mmodel_input_tensors_former\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel_input_tensors_former\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'vocabs' is not defined"
     ]
    }
   ],
   "source": [
    "NUM_TRAIN_EPOCHS = 10 #20\n",
    "SAVE_EVERY_EPOCHS = 1\n",
    "TRAIN_BATCH_SIZE = 512 #1024\n",
    "TEST_BATCH_SIZE = TRAIN_BATCH_SIZE\n",
    "TOP_K_WORDS_CONSIDERED_DURING_PREDICTION = 10\n",
    "NUM_BATCHES_TO_LOG_PROGRESS = 10\n",
    "NUM_TRAIN_BATCHES_TO_EVALUATE = 1800\n",
    "READER_NUM_PARALLEL_BATCHES = 6  # cpu cores [for tf.contrib.data.map_and_batch() in the reader]\n",
    "SHUFFLE_BUFFER_SIZE = 10000\n",
    "CSV_BUFFER_SIZE = 100 * 1024 * 1024  # 100 MB\n",
    "MAX_TO_KEEP = 10\n",
    "\n",
    "# Automatically filled by `Code2VecModelBase._init_num_of_examples()`.\n",
    "NUM_TRAIN_EXAMPLES: int = 0\n",
    "NUM_TEST_EXAMPLES: int = 0\n",
    "\n",
    "# model hyper-params\n",
    "MAX_CONTEXTS = 200\n",
    "MAX_TOKEN_VOCAB_SIZE = 13011\n",
    "MAX_TARGET_VOCAB_SIZE = 2612\n",
    "MAX_PATH_VOCAB_SIZE = 9114\n",
    "DEFAULT_EMBEDDINGS_SIZE = 128\n",
    "TOKEN_EMBEDDINGS_SIZE = DEFAULT_EMBEDDINGS_SIZE\n",
    "PATH_EMBEDDINGS_SIZE = DEFAULT_EMBEDDINGS_SIZE\n",
    "CODE_VECTOR_SIZE = PATH_EMBEDDINGS_SIZE + 2 *TOKEN_EMBEDDINGS_SIZE\n",
    "TARGET_EMBEDDINGS_SIZE = CODE_VECTOR_SIZE\n",
    "DROPOUT_KEEP_RATE = 0.75\n",
    "SEPARATE_OOV_AND_PAD = False\n",
    "\n",
    "train_steps_per_epoch=NUM_TRAIN_EXAMPLES / TRAIN_BATCH_SIZE\n",
    "\n",
    "# Automatically filled by `args`.\n",
    "PREDICT: bool = False   # TODO: update README;\n",
    "MODEL_SAVE_PATH: Optional[str] = None\n",
    "MODEL_LOAD_PATH: Optional[str] = None\n",
    "TRAIN_DATA_PATH_PREFIX: Optional[str] = None\n",
    "TEST_DATA_PATH: Optional[str] = ''\n",
    "RELEASE: bool = False\n",
    "EXPORT_CODE_VECTORS: bool = False\n",
    "SAVE_W2V: Optional[str] = None   # TODO: update README;\n",
    "SAVE_T2V: Optional[str] = None   # TODO: update README;\n",
    "VERBOSE_MODE: int = 0\n",
    "LOGS_PATH: Optional[str] = None\n",
    "DL_FRAMEWORK: str = 'tensorflow'  # in {'keras', 'tensorflow'}\n",
    "USE_TENSORBOARD: bool = False\n",
    "\n",
    "\n",
    "__logger: Optional[logging.Logger] = None\n",
    "    \n",
    "vocabs = vocabs\n",
    "config = config\n",
    "model_input_tensors_former = model_input_tensors_former\n",
    "estimator_action = estimator_action\n",
    "repeat_endlessly = repeat_endlessly\n",
    "CONTEXT_PADDING = ','.join([vocabs.token_vocab.special_words.PAD,\n",
    "                                         vocabs.path_vocab.special_words.PAD,\n",
    "                                         vocabs.token_vocab.special_words.PAD])\n",
    "csv_record_defaults = [[vocabs.target_vocab.special_words.OOV]] + \\\n",
    "                                   ([[CONTEXT_PADDING]] * config.MAX_CONTEXTS)\n",
    "\n",
    "# initialize the needed lookup tables (if not already initialized).\n",
    "create_needed_vocabs_lookup_tables(vocabs)\n",
    "\n",
    "_dataset: Optional[tf.data.Dataset] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "441c31ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'train_reader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_13372/1375427472.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[0minput_iterator\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mv1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake_initializable_iterator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_dataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[0minput_iterator_reset_op\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput_iterator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minitializer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[0minput_tensors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput_iterator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_next\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_reader' is not defined"
     ]
    }
   ],
   "source": [
    "print('Starting training')\n",
    "start_time = time.time()\n",
    "\n",
    "batch_num = 0\n",
    "sum_loss = 0\n",
    "multi_batch_start_time = time.time()\n",
    "num_batches_to_save_and_eval = max(int(train_steps_per_epoch * SAVE_EVERY_EPOCHS), 1)\n",
    "\n",
    "train_reader = PathContextReader(vocabs=vocabs,\n",
    "                                    model_input_tensors_former=_TFTrainModelInputTensorsFormer(),\n",
    "                                    config=config, estimator_action=EstimatorAction.Train)\n",
    "input_iterator = tf.compat.v1.data.make_initializable_iterator(train_reader.get_dataset())\n",
    "input_iterator_reset_op = input_iterator.initializer\n",
    "input_tensors = input_iterator.get_next()\n",
    "\n",
    "optimizer, train_loss = _build_tf_training_graph(input_tensors)\n",
    "saver = tf.compat.v1.train.Saver(max_to_keep=MAX_TO_KEEP)\n",
    "\n",
    "print('Number of trainable params: {}'.format(\n",
    "    np.sum([np.prod(v.get_shape().as_list()) for v in tf.compat.v1.trainable_variables()])))\n",
    "for variable in tf.compat.v1.trainable_variables():\n",
    "    print(\"variable name: {} -- shape: {} -- #params: {}\".format(\n",
    "        variable.name, variable.get_shape(), np.prod(variable.get_shape().as_list())))\n",
    "\n",
    "_initialize_session_variables()\n",
    "\n",
    "if MODEL_LOAD_PATH:\n",
    "    _load_inner_model(self.sess)\n",
    "\n",
    "sess.run(input_iterator_reset_op)\n",
    "time.sleep(1)\n",
    "print('Started reader...')\n",
    "# run evaluation in a loop until iterator is exhausted.\n",
    "try:\n",
    "    while True:\n",
    "        # Each iteration = batch. We iterate as long as the tf iterator (reader) yields batches.\n",
    "        batch_num += 1\n",
    "\n",
    "        # Actual training for the current batch.\n",
    "        _, batch_loss = self.sess.run([optimizer, train_loss])\n",
    "\n",
    "        sum_loss += batch_loss\n",
    "        if batch_num % NUM_BATCHES_TO_LOG_PROGRESS == 0:\n",
    "            _trace_training(sum_loss, batch_num, multi_batch_start_time)\n",
    "            # Uri: the \"shuffle_batch/random_shuffle_queue_Size:0\" op does not exist since the migration to the new reader.\n",
    "            # self.print('Number of waiting examples in queue: %d' % self.sess.run(\n",
    "            #    \"shuffle_batch/random_shuffle_queue_Size:0\"))\n",
    "            sum_loss = 0\n",
    "            multi_batch_start_time = time.time()\n",
    "        if batch_num % num_batches_to_save_and_eval == 0:\n",
    "            epoch_num = int((batch_num / num_batches_to_save_and_eval) * SAVE_EVERY_EPOCHS)\n",
    "            model_save_path = MODEL_SAVE_PATH + '_iter' + str(epoch_num)\n",
    "            save(model_save_path)\n",
    "            print('Saved after %d epochs in: %s' % (epoch_num, model_save_path))\n",
    "            evaluation_results = self.evaluate()\n",
    "            evaluation_results_str = (str(evaluation_results).replace('topk', 'top{}'.format(\n",
    "                self.TOP_K_WORDS_CONSIDERED_DURING_PREDICTION)))\n",
    "            self.print('After {nr_epochs} epochs -- {evaluation_results}'.format(\n",
    "                nr_epochs=epoch_num,\n",
    "                evaluation_results=evaluation_results_str\n",
    "            ))\n",
    "except tf.errors.OutOfRangeError:\n",
    "    pass  # The reader iterator is exhausted and have no more batches to produce.\n",
    "\n",
    "print('Done training')\n",
    "\n",
    "if MODEL_SAVE_PATH:\n",
    "    _save_inner_model(MODEL_SAVE_PATH)\n",
    "    print('Model saved in file: %s' % self.MODEL_SAVE_PATH)\n",
    "\n",
    "elapsed = int(time.time() - start_time)\n",
    "print(\"Training time: %sH:%sM:%sS\\n\" % ((elapsed // 60 // 60), (elapsed // 60) % 60, elapsed % 60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81d30c19",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
